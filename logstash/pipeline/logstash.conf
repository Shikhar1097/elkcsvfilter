input {
  file {
    path => "/usr/share/logstash/files/*"
    exclude => "*.conf"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  csv {
    separator => ","
    columns => ["id","answer_date","ministry","question_type","question_no","question_by","question_title","question_description","answer"]
  }
}

output {
	elasticsearch {
    hosts => "elasticsearch:9200"
    index => "data-index"
    action => "index"
    user => "elastic"
		password => "changeme"
  }

  stdout {
    codec => "rubydebug"
  }
}

# input {
#   http_poller {
#     urls => {
#       test2 => {
#         # Supports all options supported by ruby's Manticore HTTP client
#         method => get
#         url => "api/endpoint/"
#         headers => {
#     	 # //Put headers here.
#         }
#       }
#       test3 => {
#         # Supports all options supported by ruby's Manticore HTTP client
#         method => get
#         url => "api/endpoint/"
#         headers => {
#           # //Put headers here.
#         }
#       }
#     }
#     request_timeout => 60
#     # Supports "cron", "every", "at" and "in" schedules by rufus scheduler
#     # Will only run once.
#     schedule => { "in" => "0" }
#     codec => "json"
#     # A hash of request metadata info (timing, response headers, etc.) will be sent here
#     # metadata_target => "http_poller_metadata"
#   }
# }

# filter {
#   prune {
#     whitelist_names => ["result"]
#   }
#   split {
#     field => result
#   }
#   date {
#     match => ["[result][logEntryTS]" , "yyyy-MM-dd'T'HH:mm:ss.SSSZ"]
#     target => "@timestamp"
#   }
# }

# output {
#   stdout {  }
#   elasticsearch {
# 		hosts => "elasticsearch:9200"
# 		user => "elastic"
# 		password => "changeme"  
#     index => timeline_recorder
#     action => index
# 	}
# }
